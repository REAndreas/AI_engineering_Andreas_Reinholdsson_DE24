{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04f3944",
   "metadata": {},
   "source": [
    "## Gemini intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c2a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  Why did the data engineer get kicked out of the restaurant? They kept complaining about the \"dirty data\" on their plate!\n",
      "2.  How many data engineers does it take to change a lightbulb? None, they're too afraid of changing the schema and breaking the entire house's electrical grid.\n",
      "3.  A data engineer walks into a data lake... and immediately starts looking for a boat because it turned into a data swamp three months ago.\n",
      "4.  What's a data engineer's favorite type of music? Pipeline breaks – because then they get to \"fix\" the flow!\n",
      "5.  What did the data engineer say when their manager asked for \"real-time, perfectly clean, all-historical, yet minimal latency\" data? \"Is that before or after I invent a time machine and hire a team of data fairies?\"\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"Generate some funny jokes avout data engineering. Give 5 points in markdown format\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5ac56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77848fa",
   "metadata": {},
   "source": [
    "## Analyze tokens\n",
    "\n",
    "- basic units of text for LLMs\n",
    "- can be as short as one character or as long as one word\n",
    "\n",
    "The free tier in gemini API allows for (Gemini 2.5 flash)\n",
    "- Requests per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Requests per day (RDP): 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b12266a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=191,\n",
       "  prompt_token_count=17,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=17\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1291,\n",
       "  total_token_count=1499\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e84f5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f966469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tokens - number of tokens in models response\n",
      "metadata.candidates_token_count = 191\n"
     ]
    }
   ],
   "source": [
    "print(\"Output tokens - number of tokens in models response\")\n",
    "print(f\"{metadata.candidates_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49890cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in user input or user promt\n",
      "metadata.prompt_token_count = 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens in user input or user promt\")\n",
    "print(f\"{metadata.prompt_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08ca61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in used for internal thinking\n",
      "metadata.thoughts_token_count = 1291\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens in used for internal thinking\")\n",
    "print(f\"{metadata.thoughts_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958d2be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used\n",
      "metadata.total_token_count = 1499\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used\")\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97a3f8",
   "metadata": {},
   "source": [
    "## Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e83121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  A data engineer walks into a bar. The bartender asks, \"What can I get you?\" The data engineer replies, \"Just give me all your data, I'll figure out what I need later, and probably rebuild this bar from scratch with a new data model.\"\n",
      "\n",
      "2.  Why did the data engineer break up with the data scientist? Because she kept trying to *predict* his next move, and he just wanted to *transform* their relationship.\n",
      "\n",
      "3.  What's a data engineer's favorite type of music? Pipelines! They love the smooth flow and the occasional *orchestration*.\n",
      "\n",
      "4.  My boss told me to consolidate a bunch of spreadsheets. I spent three days building a robust, scalable, cloud-native data pipeline with automated schema inference and real-time monitoring. He just wanted it in one Google Sheet. I cried a little inside.\n",
      "\n",
      "5.  You know you're a data engineer when you find yourself explaining the intricate nuances of idempotency to your cat, who just stares back, unimpressed, probably thinking about their own very efficient data ingestion process (eating).\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"Generate some funny jokes avout data engineering. Give 5 points in markdown format\",\n",
    "    config = types.GenerateContentConfig(\n",
    "        thinking_config = types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d17f7e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=242,\n",
       "  prompt_token_count=17,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=17\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=259\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177613e6",
   "metadata": {},
   "source": [
    "## System instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02690682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Weather data not found. Perhaps it's a... *cloud* computing issue? Ha! Get it? Because clouds... and computing... I'll `return` to my regularly scheduled programming now.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are a joking robot called Ro Båt, which will\n",
    "always answer with a programming joke.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"What is the weather today?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = prompt,\n",
    "    config = types.GenerateContentConfig(\n",
    "        thinking_config = types.ThinkingConfig(thinking_budget=0),\n",
    "        system_instruction= system_instruction\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9414619c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=43,\n",
       "  prompt_token_count=30,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=30\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=73\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecac37c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.split()), len(system_instruction.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac3856",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79dc48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit nibbled a dandelion, its ears swiveling to catch every sound. Suddenly, it froze, then bolted into the undergrowth as a shadow passed overhead.\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"write a 2 sentence story about a gra rabbit\"\n",
    "\n",
    "boring_story = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = story_prompt,\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "        #thinking_config = types.ThinkingConfig(thinking_budget=0),\n",
    "        #system_instruction= system_instruction\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad2f6d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit twitched its nose, nibbling clover in the meadow's dappled sunlight. Suddenly, a shadow fell, and it froze, ears swiveling towards the silent hawk above.\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"write a 2 sentence story about a gra rabbit\"\n",
    "\n",
    "boring_story = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = story_prompt,\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=0.0\n",
    "        #thinking_config = types.ThinkingConfig(thinking_budget=0),\n",
    "        #system_instruction= system_instruction\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63f59108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A timid gray rabbit munched on dew-kissed clover, its ears twitching at the slightest rustle from the nearby woods. A hawk's shadow swept low, and the rabbit, now perfectly still, blended into the muted hues of the awakening field.\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"write a 2 sentence story about a gra rabbit\"\n",
    "\n",
    "creative_story = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = story_prompt,\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "        #thinking_config = types.ThinkingConfig(thinking_budget=0),\n",
    "        #system_instruction= system_instruction\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "460eddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tiny gray rabbit, all dusty fur and twitching nose, meticulously nibbled the sweetest clover at the field's edge. With a sudden thrum of warning in its ears, it vanished into a bramble patch just as a shadow streaked silently overhead.\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"write a 2 sentence story about a gra rabbit\"\n",
    "\n",
    "creative_story = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = story_prompt,\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "        #thinking_config = types.ThinkingConfig(thinking_budget=0),\n",
    "        #system_instruction= system_instruction\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128552e",
   "metadata": {},
   "source": [
    "## Multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5e528c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Description of the Dude\n",
      "\n",
      "This image presents a cartoon-style illustration of a young man, likely in his late teens or twenties, intensely focused on a laptop.\n",
      "\n",
      "## Appearance:\n",
      "*   **Hair:** Short, dark, and somewhat spiky or textured on top, styled casually.\n",
      "*   **Face:** He has a light to medium skin tone, and wears black-framed rectangular glasses. His dark, thin eyebrows are slightly furrowed, conveying an expression of concentration or mild thoughtfulness. His mouth is a simple, straight line, indicating a neutral or slightly pensive mood.\n",
      "*   **Build:** His visible upper body suggests a lean or average build.\n",
      "\n",
      "## Attire:\n",
      "*   He is wearing a raglan-style baseball t-shirt. The sleeves are black, and the body of the shirt is a cream or light beige color.\n",
      "*   The most distinctive feature is the graphic on his shirt:\n",
      "    *   At the top, there's a classic bell curve shape labeled \"NORMAL DISTRIBUTION\".\n",
      "    *   Below it, a sad, ghost-like figure with a similar bell curve outline is labeled \"PARANORMAL DISTRIBUTION\". This clever pun highlights an interest in statistics, data, or science, combined with a sense of humor.\n",
      "\n",
      "## Activity & Setting:\n",
      "*   He is shown actively typing on a silver laptop, which is clearly identifiable as an Apple MacBook due to the prominent Apple logo on its lid. His left hand is visible, positioned over the keyboard.\n",
      "*   His gaze is directed intently towards the laptop screen, reinforcing his focused demeanor.\n",
      "*   The background is a plain, dark blue-grey, putting full emphasis on the character and his immediate activity.\n",
      "\n",
      "## Inferred Personality/Interests:\n",
      "*   Based on his attire (the statistical pun shirt) and his engagement with a laptop, he likely has interests in fields such as mathematics, statistics, data science, programming, or technology.\n",
      "*   His focused expression suggests he is intelligent, analytical, and perhaps a bit reserved or studious. He clearly appreciates a good, nerdy pun!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = {\"parts\": [\n",
    "        {\"text\": \"Tell me about this dude, write in markdown format\"},\n",
    "        {\"inline_data\": {\n",
    "            \"mime_type\": \"image/png\",\n",
    "            \"data\": open(\"assets/kokchun.png\", \"rb\").read()\n",
    "        }}\n",
    "    ]\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6131c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"exports\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5fecbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exports/image_dedcription.md\", \"w\") as file:\n",
    "    file.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-andreas-reinholdsson-de24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
